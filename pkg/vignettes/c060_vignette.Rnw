% setwd("C:/Projekte/explained variation/peperr/CSDA/c060/pkg/vignettes")
% setwd("~/Projekte_StatsMethods/c060_workcopy/pkg/vignette")
% library(cacheSweave)
% Sweave("c060_vignette.Rnw", driver = cacheSweaveDriver)
% tools::texi2dvi("c060_vignette.tex", pdf=TRUE)

\documentclass[preprint, 11pt, authoryear]{elsarticle}

\usepackage{graphicx}
\usepackage{mathptmx}      % use Times fonts if available on your TeX system
\usepackage[authoryear]{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{color}
\usepackage{pst-all}

% please place your own definitions here and don't use \def but
% \newcommand{}{}
\newcommand{\Scmd}[1]{\texttt{#1}} % For R/ S commands
\newcommand{\R}{R}

\definecolor{red}{rgb}{1,0.4,0.4}
\definecolor{blue}{rgb}{0.4,0.4,1}
\definecolor{green}{rgb}{0,0.5,0}
\definecolor{gradbegin}{rgb}{0.6,0.1,0.6} % purple
\newcommand{\commentMZ}[1]{\textit{\textbf{\red[Manuela: #1]}}}

\journal{Computational Statistics \& Data Analysis}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for the associated footnote;
%% use the fnref command within \author or \address for footnotes;
%% use the fntext command for the associated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for the associated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%%
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \address{Address\fnref{label3}}
%% \fntext[label3]{}

\setkeys{Gin}{width=\linewidth}% general figure width
\SweaveOpts{prefix.string=figures/fig, eps=FALSE}
%Because eps=FALSE, all graphics are created as pdf graphics only. This implies that the resulting tex document needs to be compiled with pdflatex rather than with latex.

\title{C060 Vignette}
\author{Martin Sill, Thomas Hielscher, Natalia Becker, Manuela Zucknick}

\address{}

%% use optional labels to link authors explicitly to addresses:
%% \author[label1,label2]{<author name>}
%% \address[label1]{<address>}
%% \address[label2]{<address>}

\begin{abstract}
%% Text of abstract

\end{abstract}

\begin{keyword}
%% keywords here, in the form: keyword \sep keyword
%% MSC codes here, in the form: \MSC code \sep code
%% or \MSC[2008] code \sep code (2000 is the default)
\end{keyword}

\end{frontmatter}

\section{Introduction}
\label{intro}

Penalized regression models provide a statistically appealing method to build prediction models from high-dimensional data sources. Since the 
introduction of the lasso for linear regression models \citep{tibshirani96}, the methodology has been extended to generalized linear regression models,
time-to-event endpoints \citep{tibshirani97} etc. In addition to the well-known $L_1$- (lasso) and $L_2$-norms (ridge) penalty functions, various other penalties have been proposed in recent years to select features and/or estimate their effects. In particular, we will use the elastic net penalty function \citep{zou05}, which is a linear combination of the $L_1$- and $L_2$-norms.

With ever increasing data, the properties of the algorithm to actually fit the model have become almost as important as the statistical model itself. 
In 2010, Friedman, Hastie and Tibshirani proposed a coordinate descent algorithm \citep{FHT2010} for generalized linear regression models, which has since then been extended to penalized Cox PH regression models \citep{simon2011}. Due to its efficiency this algorithm is considered one of the state-of-the-art
approaches to estimate penalized regression models with lasso, ridge or elastic net penalty terms. 

This algorithm has also been implemented in \R{} in the \texttt{glmnet} package. The package provides functions to tune and fit regression models, 
plot the results, and make predictions. However, in practical applications, where often an independent validation data set is lacking, some additional features and routines are desirable as part of a complete data analysis. We have assembled some functions that enhance the existing functionality of the \texttt{glmnet} package or
allow to use it within the framework of other existing \R{} packages. These functions have been useful in our daily work here at the German Cancer Research 
Center where prognostic modelling of patient survival data is of particular interest. Therefore, for illustration purposes we focus on penalized Cox PH regression models in this article. But most of the functions are applicable to all types of regression models implemented in the \texttt{glmnet} package.

We provide \R{} functions to perform stability selection \citep{MeinshausenBuehlmann2010} in a computationally efficient way using \texttt{glmnet} which allows to select the most stable features at a given error level. We also provide an approach to select the optimal parameter combination ($\alpha$, $\lambda$) for elastic net penalties using an interval-search algorithm \citep{froehlich2005} which is often faster and more accurate than a standard grid search (Ref for a comparison study?). Another very useful addition for real-life applications of \texttt{glmnet} for building prognostic models is the provision of wrapper functions to allow the computation of resampling-based prediction error curves with the framework of the \R{} package \texttt{peperr} \citep{Porz2009}. The \texttt{peperr} package makes it computationally feasible to assess the
predictive accuracy of a penalized Cox PH regression model via resampling methods even for very large-scale applications by employing parallel computing. We also provide the possibility to spead up stability selection by parallel computing using the functionalities of the \R{} base package \texttt{parallel}.

\section{Data application}
Throughout this article we use the gene expression data set on cytogenetically normal acute myeloid leukemia (CN-AML) by \citet{metzeler08} and corresponding clinical data in order to illustrate a typical application of penalized Cox PH regression models with the aim of developing a prognostic model for patient survival while at the same time identifying the most influential gene expression features. To simulate the typical situation that only one data set is available for model training and evaluation, we only use the data set that was used as validation data in the original publication by \citet{metzeler08}. The data can be accessed from the Gene Expression Omnibus (GEO) data repository (\url{http://www.ncbi.nlm.nih.gov/geo}) by the National Center for Biotechnology Information (NCBI). We find the Metzeler \textit{et al.} data set under GEO accession number GSE12417.

The data set contains gene expression data for 79 patient samples measured with Affymetrix HG-U133 Plus 2.0 microarrays. The median survival time of these 79 patients was 17.6 months with 40\% censoring.

\section{Methods and algorithms}
\commentMZ{What we need here is not just the description of the statistical methods, but also of the specific algorithms that are implemented in the R package.}

\subsection{Penalized generalized linear models and Cox models}\label{methods:penalized}
An efficient implementation for fitting generalized linear models and Cox proportional hazards models with regularization by the lasso or elastic net penalty terms is provided by the \R{} package \texttt{glmnet} \citep{FHT2010, simon2011}. This implementation uses a coordinate descent algorithm for fitting the models for a specified penalty parameter value $\lambda$. The computation of an entire regularization path across a range of values $\{\lambda_1,\lambda_2...,\lambda_k\}$ with \texttt{glmnet} is very fast, because previously computed solutions for $\{\lambda_1,...,\lambda_{j-1}\}$ are used as a 'hot' starting value for the computation of $\lambda_j$.

Models are fitted by maximizing the penalized log-likelihood function for generalized linear models and the penalized partial log-likelihood for Cox models. Cross-validation can be performed to decide which model (i.e. which penalty parameter values) to choose by using the cross-validated (partial) log-likelihood as the loss function.
\commentMZ{Is the approach by \citet{verweij93} used for Cox models?}
\commentMZ{The remainder of the text in this section \ref{methods:penalized} is from the Benner et al. (2010) paper and needs to be rewritten and adapted to this manuscript.}

The penalized (partial) log-likelihood is given by
\begin{equation}\label{pen.part.likelihood}
l_n(\boldsymbol\beta)- \sum_{j=1}^p  p_{\lambda^*}(|\beta_j|)
\end{equation}
where $l_n(\boldsymbol\beta)$ denotes the (partial) log-likelihood given $n$ observations.
The dimension of the parameter vector $\boldsymbol\beta$ is $p$ and $p_\lambda(|\cdot|)$ is the penalty function with tuning parameter $\lambda^*$. Correspondingly, the objective function employed in \texttt{glmnet} is (with $\lambda = \lambda^*/n$):
\begin{equation}\label{obj.fun}
- l_n(\boldsymbol\beta)/n + \sum_{j=1}^p  p_\lambda(|\beta_j|).
\end{equation}

\subsection{$L_2$-penalized Cox regression}
Penalized maximum likelihood estimation in Cox regression with the ridge penalty
\begin{equation}\label{ridge}
p_\lambda(|\beta|)=\lambda\beta^2
\end{equation}
was introduced by \citet{verweij94}.
Ridge penalized regression does not perform model selection and results in downwardly biased parameter estimates.
On the other hand, it has been found to produce models with good prediction performance in high-dimensional genomic
applications \citep[e.g.][]{bovelstad07}.

\subsection{$L_1$-penalized Cox regression}
\Citet{tibshirani97} proposed to use an $L_1$-penalized Cox model with
\begin{equation}
p_\lambda(|\beta|)=\lambda | \beta |
\end{equation}
and described a technique, called the lasso for ''least absolute shrinkage and selection operator'', for parameter estimation.
The $L_1$-penalty has the advantage over the $L_2$-penalty of shrinking some of the coefficients to zero, i.e. it has an in-built variable selection procedure.

\subsection{The elastic net}
\Citet{zou05} introduced the elastic net, which employs a combination of the $L_1$- and $L_2$-penalty
\begin{equation}\label{enet}
p_{\lambda_1,\lambda_2}(|\beta|)=\lambda_1 | \beta | + \lambda_2 \beta ^2.
\end{equation}
\Citet{zou05} rescale the initial solutions from the optimization of the doubly-penalized log-likelihood function by the factor $1 + \lambda_2$, in order to reduce the effect of the double shrinkage.
The elastic net performs variable selection and parameter estimation equivalently to the lasso.
But the additional $L_2$-penalty term distributes the weight to more variables, such that the elastic net tends to select more variables than the lasso. 
This is especially the case in situations with high correlation, where the lasso would select only one variable of a set of highly correlated variables.

Throughout this manuscript we use an alternative parametrization of the elastic net penalty function equivalently to the formulation used in the \texttt{glmnet} package:
\begin{equation}\label{enet2}
p_{\alpha,\lambda}(|\beta|)= \lambda\times ( (1-\alpha)\frac{1}{2} |\beta|^2 + \alpha |\beta|).
\end{equation}

\subsubsection{The interval-search algorithm to select the optimal elastic net parameter combination}

\subsection{Stability selection}
\subsection{Prediction error curves for survival models}

<<setup, echo=F, results=hide>>=
rm(list=ls())

# load libraries
library(cacheSweave)
library(Biobase)
library(genefilter)
library(ggplot2)
library(GEOquery)
library(limma)
library(xtable)

library(glmnet)
require(penalizedSVM)
library(parallel) #for stabilityselection.R
library(peperr) #for peperr_glmnet.R
library(tgp) #for EPSGO.R
library(mlegp) #for EPSGO.R
library(pamr) #for tune.glmnet.interval.R

# R functions required
source("../R/peperr_glmnet.r")
source("../R/stabilityselection.R")
source("../R/tune_glmnet_interval.R")
source("../R/EPSGO.R")

dir.create("figures",showWarnings = FALSE)

# set cache folder; needs different driver, to run as:
# Sweave("c060_vignette.Rnw",driver=cacheSweaveDriver)
setCacheDir("cache")
@

<<GEOdataGSE12417, echo=F, results=hide>>=
if (file.exists("Rdata/expressionSet.Rdata")) {
    load("Rdata/expressionSet.Rdata") 
} else {
    library(GEOquery)
    
    dir.create("GEOdata",showWarnings = FALSE)
    dir.create("Rdata",showWarnings = FALSE)
    
    # download processed data files from url or if available get data from locally saved copies
    geo_exprs_data  <- getGEO("GSE12417", destdir="GEOdata",AnnotGPL=T)[[1]]
    annotation(geo_exprs_data) <- "hgu133plus2"
    
    #clinical characteristics including survival data are in "characteristics_ch1" in the phenoData 
    clin.df <- as.data.frame(strsplit2(as.character(pData(geo_exprs_data)$characteristics_ch1), split=";"))
    clin.df[,1] <- strsplit2(clin.df[,1]," ")[,7]
    clin.df[,2] <- as.numeric(sub("=", "", strsplit2(clin.df[,2]," ")[,3]))
    clin.df[,3] <- as.numeric(strsplit2(clin.df[,3]," ")[,4])
    clin.df[,4] <- as.numeric(strsplit2(clin.df[,4]," ")[,4])
    colnames(clin.df) <- c("FAB", "age", "os", "os_status")
    rownames(clin.df) <- rownames(pData(geo_exprs_data))
    pData(geo_exprs_data) <- clin.df

    save(geo_exprs_data, file="Rdata/expressionSet.Rdata")  
    unlink("GEOdata", recursive=TRUE) #delete GEOdata
} 
@

<<preprocessData, echo=F, results=hide>>=
# unspecific filtering: select most varying probe sets
rowvars    <- rowVars(exprs(geo_exprs_data))
topprobes   <- which(rowvars>=sort(rowvars,decreasing=T)[10000])
eset       <- geo_exprs_data[topprobes,]     
@

\clearpage
\section{Application and demonstration of software}
\subsection{Starting off: Lasso-penalised Cox model} 

\begin{figure}
<<glmnet, echo=F, results=hide, fig=TRUE, height=4>>=
set.seed(1234)
y <- cbind(time=pData(eset)$os, status=pData(eset)$os_status)
cvres <- cv.glmnet(y=y, x=t(exprs(eset)), family="cox", nfolds=10)
res <- cvres$glmnet.fit
plot(cvres)

cof <- coef(res, s=cvres$lambda.min)
names.cof <- rownames(cof)
cofn <- cof[which(cof!=0)]
names(cofn) <- names.cof[which(cof!=0)]
@
\caption{\label{fig:cvlasso}Cross-validated partial log-likelihood function, including upper and lower standard deviations, as a function of $\log{\lambda}$ for the AML data set.}
\end{figure}

We can apply the \texttt{glmnet} function to fit a lasso-penalized Cox model to the CN-AML data set. The function call with default penalty parameter settings will fit the lasso model for 100 $\lambda$ data derived values:
<<glmnet0, echo=TRUE, results=hide>>=
fit <- glmnet(y=y, x=t(exprs(eset)), family="cox")
@
 
We tune the lasso penalty parameter by 10-fold cross-validation using the \texttt{cv.glmnet} function. The loss function, i.e. the cross-validated partial log-likelihood, is shown in Figure \ref{fig:cvlasso} including upper and lower standard deviations as a function of $\log{\lambda}$ for the AML data set. The penalty parameter value minimizing the loss function is $\lambda =$\Sexpr{round(cvres$lambda.min, 3)} and corresponds to a final lasso model with the following \Sexpr{length(cofn)} selected features:
<<glmnet2, echo=F, results=verbatim>>=
print(cofn)
#cofn.ix <- predict(res, type="nonzero", s=cvres$lambda.min)
@

The selected features are highlighted as red lines in the coefficient paths shown in Figure \ref{fig:coefpath}. The coefficient path shows the development of the regression coefficient estimates with increasing regularization.

\begin{figure}
<<glmnet3, echo=F, results=hide, fig=TRUE, height=7, width=7>>=
bet <- res$beta[match(names(cofn), rownames(res$beta)),]
par(mar=c(4,4,2.5,1), mgp=c(2.5,1,0), mfrow=c(2,2))

plot(res, xvar="lambda", col="gray")
glmnet:::plotCoef(bet, lambda = res$lambda, df = res$df, dev = res$dev.ratio, xvar = "lambda", add=TRUE, col="red")
abline(v=log(cvres$lambda.min), lty=3)
abline(v=log(cvres$lambda.1se), lty=3)

glmnet:::plotCoef(bet, lambda = res$lambda, df = res$df, dev = res$dev.ratio, xvar = "lambda", add=FALSE, col="red")
abline(v=log(cvres$lambda.min), lty=3)
abline(v=log(cvres$lambda.1se), lty=3)

norm <- apply(abs(res$beta), 2, sum)
plot(res, xvar="norm", col="gray")
glmnet:::plotCoef(bet, xvar = "norm", add=TRUE, col="red",
                  norm = norm, lambda = res$lambda, df = res$df, dev = res$dev.ratio)
abline(v=norm[match(cvres$lambda.min, cvres$lambda)], lty=3)
abline(v=norm[match(cvres$lambda.1se, cvres$lambda)], lty=3)

plot(res, xvar="dev", col="gray")
glmnet:::plotCoef(bet, lambda = res$lambda, df = res$df, dev = res$dev.ratio, xvar = "dev", add=TRUE, col="red")
abline(v=res$dev.ratio[match(cvres$lambda.min, cvres$lambda)], lty=3)
abline(v=res$dev.ratio[match(cvres$lambda.1se, cvres$lambda)], lty=3)
@
\caption{\label{fig:coefpath}Coefficient paths for lasso penalised Cox PH regression model applied to the AML data set.}
\end{figure}

%\clearpage
%At this point we would like to assess the prediction performance of the lasso model. We can do this with bootstrapped prediction error curves and corresponding integrated Brier score values (see Thomas' functions adapted for peperr/pec).

%Once we have seen that this model is not very satisfactory, we can attempt to improve the model in two ways. First, we can fit an elastic net model rather than lasso (and use Natalia's search algorithm for that). And second, we can assess the stability of the lasso (and elastic net) models by stability selection and identify the most stable features (using Martin's stabilityselection.R script).

\subsection{Resampling based prediction errors}
\label{pec}

Once the final prognostic model is selected, we need to assess its prediction accuracy for future patients, frequently also in comparison
with established clinico-pathological prognostic markers. In many applications no independent validation data set is available.
The same data set need to be used to develop and assess the prognostic model. This is even more problematic for high-dimensional data,
where the risk of overfitting is much more present. Resampling-based methods can be used to unbiasedly estimate the predictive accuracy 
of the prognostic model in this situation. This is also called internal validation or pre-validation.

The \texttt{R} package \texttt{peperr} \citep{Porz2009} provides a modular framework for survival and binary endpoints, i.e. prognostic and
classification models. Wrapper functions for new or customized prediction model algorithms can be defined and passed to the generic call function \texttt{peperr}.  
In case of prognostic models, algorithm specific wrapper functions for model fitting, tuning and prediction are required.
Wrapper functions for selected machine learning approaches are already implemented.

Prediction accuracy is per default assessed with prediction error curves based on the time-dependent Brier score \citep{graf99}. 
But it is also possible to define and use customized accuracy measures.

We defined additional wrapper functions for the \texttt{glmnet} algorithm for fitting (\texttt{fit.glmnet}) and tuning (\texttt{complexity.glmnet}) the model,
and predicting survival probabilities (\texttt{predictProb.glmnet}) based on the fitted model and the estimated baseline hazard from the training data. 

We estimate the $L_1$-penalized Cox PH regression model for overall survival starting with the 10.000 most varying probe sets using \texttt{glmnet}. 
The $.632+$ bootstrap estimator is calculated based on subsampling \citep{BS2008} using only 20 bootstrap samples for illustration.

<<peperr, echo=T, results=hide, cache=T>>=
peperr_obj <- peperr(response=Surv(eset$os, eset$os_status), x=t(exprs(eset)),
                     fit.fun=fit.glmnet,args.fit=list(standardize=F, family="cox"),
                     complexity=complexity.glmnet, args.complexity=list(standardize=F, family="cox", nfolds=10),
                     trace=F, RNG="fixed",seed=0815, 
                     indices=resample.indices(n=dim(eset)[2], sample.n = 20, method = "sub632"))
@

Individual bootstrap results can be visualized with the \texttt{plot.peperr} function from the \texttt{peperr} package showing the selected complexity parameters, 
out-of-bag prediction error curves as well as the prediction error integrated over time, and the predictive partial log-likelihood (PLL) values. In order to calculate the predictive PLL values again an algorithm specific wrapper (here \texttt{PLL.coxnet}) needs to be defined.

In addition, we provide a slightly modified version of the prediction error curves plot function from the \texttt{peperr} package which allows to display the number still at risk (\texttt{plot.peperr.curves}) as shown in figure \ref{fig:pec1}. 

\begin{figure}
<<peperrPlot, echo=T, results=hide, fig=TRUE, width=12, height=10>>=
plot.peperr.curves(peperr_obj, at.risk=T)
@
\caption{Prediction error curves}
\label{fig:pec1}
\end{figure}

The \texttt{peperr} package is designed for high-dimensional covariates data and allows for various types of parallel computations.
Here, we re-run the calculations on 3 CPUs in parallel using a socket cluster on a Windows OS.

<<peperrParallel, echo=T, results=hide, cache=T>>=
peperr_obj_parallel <- peperr(response=Surv(eset$os, eset$os_status), x=t(exprs(eset)),
                              fit.fun=fit.glmnet,args.fit=list(standardize=F, family="cox"),
                              complexity=complexity.glmnet, args.complexity=list(standardize=F, family="cox", nfolds=10),
                              trace=F, RNG="fixed",seed=0815, cpus=3, parallel=T, clustertype="SOCK", 
                              load.list=list(functions=c("basesurv")),
                              indices=resample.indices(n=dim(eset)[2], sample.n = 20, method = "sub632"))
@

<<peperrParallelPlot, echo=F, results=hide, fig=F, width=12, height=10>>=
plot.peperr.curves(peperr_obj_parallel, at.risk=T)
@

Additional arguments can be passed directly to the \texttt{glmnet} call by specifing additional arguments for the fitting or tuning procedure.
Here, we include patient's age as mandatory model variable into the prognostic model, i.e. age is not subject to penalization.

<<peperrMandatoryParallel, echo=T, results=hide, cache=T>>=
peperr_obj_parallel_mandatory  <- peperr(response=Surv(eset$os, eset$os_status), x=data.frame(eset$age,t(exprs(eset))),
                                          fit.fun=fit.glmnet,args.fit=list(standardize=F, family="cox", 
                                                                           penalty.factor=rep(0:1,times=c(1,dim(eset)[1]))),
                                          complexity=complexity.glmnet, args.complexity=list(standardize=F, nfolds=10, family="cox",
                                                                                             penalty.factor=rep(0:1,times=c(1,dim(eset)[1]))),
                                          trace=F, RNG="fixed",seed=0815, cpus=3, parallel=T, clustertype="SOCK", 
                                          load.list=list(functions=c("basesurv")),
                                          indices=resample.indices(n=dim(eset)[2], sample.n = 20, method = "sub632"))
@

<<peperrMandatoryParallelPlot, echo=F, results=hide, fig=F, width=12, height=10>>=
lines(peperr_obj$attribute,perr(peperr_obj,"632p"),lty=1,col=6)
@

For classification models, the same wrapper functions for fitting and tuning the model are called.
Model performance measures shipped with the \texttt{peperr} packages are misclassification rate and Brier score.

We  extended functionality of the Brier score (\texttt{aggregation.brier}) and misclassification rate (\texttt{aggregation.misclass}) 
calculation for the \texttt{glmnet} algorithm, and defined AUC under the ROC curve (\texttt{aggregation.auc}) as additional performance measure.
For binary responses, the \texttt{peperr} package does not quite provide the same modular flexibility 
as for time-to-event endpoints. The predicted class probability is calculated within the performance/aggregation function by calling the algorithm specific predict function.
Whenever a new algorithm is incorporated the aggregation function has to be modified and overwritten accordingly.

For illustration purpose only we use survival status as binary response variable and build the $L_1$-penalized logistic regression model with \texttt{glmnet}.
All three prediction accuracy measures are calculated. Again, computations are run in parallel.
Results of 20 bootstrap runs are displayed in plain boxplots (figure \ref{fig:pebin1}).

<<peperrBinaryParallel, echo=F, results=hide, cache=T>>=
peperr_obj_parallel_binary_brier  <- peperr(response=eset$os_status, x=t(exprs(eset)),
                                            fit.fun=fit.glmnet,args.fit=list(standardize=F, family="binomial"),
                                            complexity=complexity.glmnet, args.complexity=list(standardize=F, family="binomial", nfolds=10),
                                            trace=F, RNG="fixed",seed=0815, cpus=3, parallel=T, clustertype="SOCK", 
                                            aggregation.fun=aggregation.brier,
                                            indices=resample.indices(n=dim(eset)[2], sample.n = 20, method = "sub632"))

peperr_obj_parallel_binary_misclass  <- peperr(response=eset$os_status, x=t(exprs(eset)),
                                               fit.fun=fit.glmnet,args.fit=list(standardize=F, family="binomial"),
                                               complexity=complexity.glmnet, args.complexity=list(standardize=F, family="binomial", nfolds=10),
                                               trace=F, RNG="fixed",seed=0815, cpus=3, parallel=T, clustertype="SOCK", 
                                               aggregation.fun=aggregation.misclass,
                                               indices=resample.indices(n=dim(eset)[2], sample.n = 20, method = "sub632"))

peperr_obj_parallel_binary_auc  <- peperr(response=eset$os_status, x=t(exprs(eset)),
                                          fit.fun=fit.glmnet,args.fit=list(standardize=F, family="binomial"),
                                          complexity=complexity.glmnet, args.complexity=list(standardize=F, family="binomial", nfolds=10),
                                          trace=F, RNG="fixed",seed=0815, cpus=3, parallel=T, clustertype="SOCK", 
                                          aggregation.fun=aggregation.auc,
                                          indices=resample.indices(n=dim(eset)[2], sample.n = 20, method = "sub632"))
@

\begin{figure}
<<peperrBinaryPlot, echo=F, results=hide, fig=TRUE, width=12, height=10 >>=
bootsamples  <- data.frame(type  = rep(c("Brier Score","Misclassification Rate","AUC"),each=20),
                           error = c(unlist(peperr_obj_parallel_binary_brier$sample.error),
                                     unlist(peperr_obj_parallel_binary_misclass$sample.error),
                                     unlist(peperr_obj_parallel_binary_auc$sample.error)))

hline_data   <- data.frame(type  = rep(c("Brier Score","Misclassification Rate","AUC"),each=4),
                           error= rep(c("mean\nout-of-bag",".632plus","apparent","null model"),3),
                                     # aggregated Brier score
                           at    = c(perr(peperr_obj_parallel_binary_brier,"resample"),
                                     perr(peperr_obj_parallel_binary_brier,"632p"),
                                     perr(peperr_obj_parallel_binary_brier,"app"),
                                     perr(peperr_obj_parallel_binary_brier,"nullmodel"),
                                     # aggregated misclassification
                                     perr(peperr_obj_parallel_binary_misclass,"resample"),
                                     perr(peperr_obj_parallel_binary_misclass,"632p"),
                                     perr(peperr_obj_parallel_binary_misclass,"app"),
                                     perr(peperr_obj_parallel_binary_misclass,"nullmodel"),
                                     # aggregated AUC
                                     perr(peperr_obj_parallel_binary_auc,"resample"),
                                     perr(peperr_obj_parallel_binary_auc,"632p"),
                                     perr(peperr_obj_parallel_binary_auc,"app"),
                                     perr(peperr_obj_parallel_binary_auc,"nullmodel")))

p    <- ggplot(bootsamples, aes(type,error))
pg   <- p + geom_boxplot(outlier.colour = rgb(0,0,0,0), outlier.size=0) + geom_jitter(position=position_jitter(width=.05)) + 
        theme_bw() + scale_y_continuous("") +  scale_x_discrete("") + facet_wrap(~ type, scales="free") +
        geom_hline(aes(yintercept=at, x=type, colour=error), hline_data, show_guide=TRUE) + opts(legend.position = "bottom")
print(pg)
@
\caption{Different bootstrap performance measures for binary classification}
\label{fig:pebin1}
\end{figure}

\clearpage
\subsection{Stability selection}

\begin{figure}
<<stabilitySelection, echo=F, results=hide, fig=TRUE>>=
set.seed(1234)
y <- cbind(time=pData(eset)$os, status=pData(eset)$os_status)
### parallel computing not enabled on Windows
cores <- 2; if (sessionInfo()$platform=="x86_64-pc-mingw32/x64 (64-bit)") cores <- 1
res <- stability.path(y=y, x=t(exprs(eset)), weakness=1, mc.cores=cores, family="cox")
par(mar=c(4,4,2,1), mgp=c(2.5,1,0))
sel <- plot.stabpath(res, fwer=0.2, pi_thr=0.5, xvar="lambda", col.all="gray")
#sel <- plot.stabpath(res, fwer=0.2, pi_thr=0.5, xvar="norm")
#sel <- plot.stabpath(res, fwer=0.2, pi_thr=0.5, xvar="dev")
@
\caption{\label{fig:stabpath}Coefficient and stability paths for lasso penalised Cox PH regression model applied to the AML data set.}
\end{figure}

Stable features (with $\hat\Pi > 0.5$ at $\lambda =$\Sexpr{round(sel$lambda, 3)}) are:
<<stabilitySelection2, echo=F, results=verbatim>>=
print(sel$stable)
@

\clearpage
\subsection{Elastic net penalised Cox PH regression model}

\subsubsection{Tune both $\lambda$ and $\alpha$ by interval search}

<<interval_search_cox_setup, echo=F, results=hide>>=  
  x <- t(exprs(eset))
  y <- cbind(time=pData(geo_exprs_data)$os,status=pData(geo_exprs_data)$os_status)
  
  Q.func <- "tune.glmnet.interval"
  
  # bounds for alpha [0,1]
  bounds <- t(data.frame(alpha=c(0, 1)))
  colnames(bounds) <- c("lower", "upper")  
  parms.coding <- "none"
  seed <- 1234
  type.measure <- 'deviance'
    
  family <- "cox"
  nfolds <- 10
  show <- "none"
  fminlower <- -100  
  type.min <- "lambda.1se"
  verbose <- TRUE
@

The task is to find such a setting of tuning parameters $\left(\alpha,\lambda\right)$, for which the \Sexpr{nfolds}-fold cross validation error of the model is minimal.
Instead of using a fixed grid, an interval search approach is applied. REFF 

The parameter space of tuning parameters is defined as follows:

<<interval_search_cox_pre, echo=F>>=
print(bounds)
@
The second tuning parameter $\lambda$ will be found for each given $\alpha$ via regularization path.  Thus, the two dimensional parameter space has the form $\left(0, 1\right) \times \mathbb{R}$. The seed is \Sexpr{seed}. For each given $\alpha$ an optimal lambda is definded as largest value of lambda such that error is within 1 standard error of the minimum. (parameter  type.min = 'lambda.1se').

<<interval_search_cox, echo=F, results=hide, cache=TRUE>>=
  # fix folds for each model
  set.seed(seed)
  foldid <- my.balanced.folds(class.column.factor=y[,2], cross.outer=nfolds)
  
  print("start interval search")
  fit <- EPSGO(Q.func, 
             bounds=bounds, 
             parms.coding=parms.coding, 
             show=show, N=NULL,   
             seed=seed, 
             fminlower=fminlower,
             # Q.func arguments
  					 x=x, y=y, family=family, 
             #nfolds=nfolds,  
             foldid=foldid,
             type.min=type.min,
             type.measure=type.measure,
             verbose=verbose)
  names(fit) 
 
  print("chose the model with min num of FS ")      
  print("# FS: ")
  sel.models <- sapply(fit$model.list, "[", "model") [fit$Ytrain == fit$fmin ]
  sel.alpha <- fit$xmin
  sel.error <- fit$fmin		
  		
  out <- list(model=sel.models, alpha=sel.alpha, error=sel.error) 
  save(fit, out, file="Rdata/fit_interval_search.RData") 
@

<<interval_search_cox_extract, echo=F, results=hide>>=
# 
alphas <- fit$Xtrain[,1]
lambdas <- unlist(sapply(sapply(fit$model, "[", 2), "[", 5))
deviances <- fit$Ytrain
# number of selected features in the models; dfs
n.features <- unlist(sapply(sapply(fit$model, "[", 2), "[", 3))

opt.alpha <- out$alpha
opt.lambda <- out$model[[1]]$lambda

summary_int_search <- as.data.frame(cbind(alpha=alphas,lambda=lambdas,deviance=deviances,n.features=n.features))
rownames(summary_int_search) <- c(1:nrow(summary_int_search))
@

<<label=tab_int_search, echo=FALSE, results=tex>>=                             
toPrint <- xtable(summary_int_search[1:10,], caption = "Summary of visited points in the parameter space", label = "tab:sum_int_search", digits = c(0,5, 7, 5, 0))
print(toPrint, table.placement = "tbp", caption.placement = "top")
@
The visited points in the parameter space ($\alpha$, $\lambda$) and resulting cross-validated deviances are summarised in table~\ref{tab:sum_int_search}.

The minimal mean cross-validated deviance over the folds of \Sexpr{round(out$error,3)} is reached for optimal parameter pair $(\alpha, \lambda ) $ = (\Sexpr{round(opt.alpha,3)},\Sexpr{round(opt.lambda,3)}). 

The 'visited' points with corresponding deviance and number of selected features in each model are presented in figure~\ref{tab:sum_int_search}. 

<<interval_search_cox_fit_opt, echo=F, results=hide>>=
#select the model with optimal parameters from the object fit
# 
f <- fit$model.list[unlist(sapply(fit$model.list,"[",1)) == fit$fmin]
if (length(f)>1) print("more than one optimal model!")
#take the first model
f1 <- f[[1]]$model

# fit the model with found optimal alpha
set.seed(seed)
cvres <- cv.glmnet(y=y, x=t(exprs(eset)), family="cox", nfolds=10, alpha=opt.alpha)
res <- cvres$glmnet.fit

cof <- coef(res, s=opt.lambda)
names.cof <- rownames(cof)
cofn <- cof[which(cof!=0)]
names(cofn) <- names.cof[which(cof!=0)]                     
@

<<>>=
summary(cofn)
head(sort(cofn))
tail(sort(cofn))
@

A feature selected by the stability algorithm is in the set of selected features
<<>>=
'206932_at' %in% names.cof
@


\textbf{TODO : plots}

\begin{figure}
<<interval_search_cox_out_plot, echo=F, results=hide, fig=TRUE, height=4>>=
plot(cvres)
@
\caption{\label{fig:interval_search} \Sexpr{type.measure} as a function of log($\alpha$) for the AML data set. }
\end{figure}

% 
% 
% \begin{figure}
% <<interval_search_cox_out_plot2, echo=F, results=hide, fig=TRUE, height=4>>=
% 
% par(las=3)
% plot(cbind(alphas,deviances), ylab="Partial Likelihood Deviance", xlab="alpha", type="p")
% axis(3,at=alphas, tick = TRUE, labels=n.features,cex.axis=0.7 )
% 
% abline(v=out$alpha, col="red")
% 
% 
% 
% plot(cbind(alphas,lambdas),  xlab="alpha", ylab="lambda", type="p")
% 
% @
% \caption{\label{fig:interval_search2} \Sexpr{type.measure} as a function of $\alpha$ for the AML data set. err ewr 22222}
% \end{figure}
% 
% 
% 
%

\clearpage
\section{Conclusions}

\section{Session Information}
The version number of \R{} and packages loaded for generating the vignette were:
<<sessionInfo, echo=FALSE, results=tex>>=
toLatex(sessionInfo())
@

\clearpage
\bibliographystyle{model2-names}
\bibliography{c060}   % name your BibTeX data base

\end{document}