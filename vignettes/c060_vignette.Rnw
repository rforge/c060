\documentclass{jss}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% library(cacheSweave)
% Sweave("c060_vignette.Rnw", driver = cacheSweaveDriver)
% tools::texi2dvi("c060_vignette.tex", pdf=TRUE)
%%%%%%our stuff
\usepackage{graphicx}
\usepackage{mathptmx}      % use Times fonts if available on your TeX system
\usepackage[authoryear]{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{longtable}
\usepackage{color}
\usepackage{pst-all}

% please place your own definitions here and don't use \def but
% \newcommand{}{}
%\newcommand{\Scmd}[1]{\texttt{#1}} % For R/ S commands
\definecolor{red}{rgb}{1,0.4,0.4}
\definecolor{blue}{rgb}{0.4,0.4,1}
\definecolor{green}{rgb}{0,0.5,0}
\definecolor{gradbegin}{rgb}{0.6,0.1,0.6} % purple

\author{Martin Sill\\
\And Thomas Hielscher\\ \hspace{3.95 cm} Divison of Biostatistics \\  \hspace{4.3 cm} German Cancer Research Center
\And Natalia Becker\\
\And Manuela Zucknick\\}
\title{\pkg{c060}: Extended Inference with Lasso and Elastic-Net Regularized Cox and Generalized Linear Models}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Martin Sill, Thomas Hielscher, Natalia Becker, Manuela Zucknick} %% comma-separated
\Plaintitle{C060: Extended Inference with Lasso and Elastic-net Regularized Cox and Generalized Linear Models} %% without formatting
\Shorttitle{\pkg{c060}: Extended Inference with Regularized Cox and Generalized Linear Models} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
We have developed the R package \pkg{c060} with the aim of improving R software functionality for high-dimensional risk prediction modelling,
e.g. for prognostic modelling of survival data using high-throughput genomic data. 
Penalized regression models provide a statistically appealing way of building risk prediction models from high-dimensional data.
The popular CRAN package \pkg{glmnet} package implements an efficient algorithm for fitting penalized Cox and generalized linear models.
However, in practical applications the data analysis will typically not stop at the point where the model has been fitted. One is for example often interested in the stability
of selected features and in assessing the prediction performance of a model and we provide functions to deal with both of these tasks. Our R functions are computationally
efficient and offer the possibility of speeding up computing time through parallel computing. Another feature which can drastically reduce computing time is an efficient interval-search algorithm,
which we have implemented for selecting the optimal parameter combination for elastic net penalties. These functions have been useful in our daily work at the German Cancer Research 
Center where prognostic modelling of patient survival data is of particular interest.
Although we focus on a survival data application of penalized Cox models in this article, the functions in our R package are applicable to all types of regression models implemented in the \pkg{glmnet} package.

}
\Keywords{\pkg{glmnet}, penalized log-likelihood method, stability selection, interval search, prediction error}
\Plainkeywords{glmnet, penalized log-likelihood method, stability selection, interval search, prediction error} %% without formatting
%% at least one keyword must be supplied

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Martin Sill\\
  Division of Biostatistics\\
  DKFZ\\
  German Cancer Research Center\\
  69120 Heidelberg, Germany\\
  E-mail: \email{m.sill@dkfz.de}\\
  URL: \url{http://www.dkfz.de/en/biostatistics}
}
%% It is also possible to add a telephone and fax number
%% before the e-mail in the following format:
%% Telephone: +43/512/507-7103
%% Fax: +43/512/507-2851

%% for those who use Sweave please include the following line (with % symbols):
%% need no 
\usepackage{Sweave}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}
\SweaveOpts{concordance=TRUE}


\section{Introduction}
\label{intro}

Penalized regression models provide a statistically appealing method to build prediction models from high-dimensional data sources, where it is the aim to simultaneously select features and to fit the model \citep{fan2010,benner2010}. Since the introduction of the lasso for linear regression models \citep{tibshirani96}, the methodology has been extended to generalized linear regression models and time-to-event endpoints \citep{tibshirani97} among others. In addition to the well-known $L_1$- (lasso) and $L_2$-norm (ridge) penalty functions, various other penalties have been proposed in recent years to select features and/or estimate their effects. In particular, we will use the elastic net penalty function \citep{zou05}, which is a linear combination of the $L_1$- and $L_2$-norms.

With ever increasing data, the properties of the algorithm used for fitting the model have become almost as important as the statistical model itself. 
In 2010, Friedman et al. proposed a coordinate descent algorithm \citep{FHT2010} for generalized linear regression models, which has since then been extended to penalized Cox proportional hazards (PH) regression models \citep{simon2011}. Due to its efficiency this algorithm is considered one of the state-of-the-art
approaches to estimate penalized regression models with lasso, ridge or elastic net penalty terms, especially in high-dimensional data scenarios. 

This algorithm has been implemented in \proglang{R} \citep{R11} in the \pkg{glmnet} package. The package provides functions to tune and fit regression models, 
plot the results, and make predictions. However, in practical applications, where often an independent validation data set is lacking, some additional features and routines are desirable as part of a complete data analysis. We have assembled some functions that enhance the existing functionality of the \pkg{glmnet} package or
allow to use it within the framework of other existing \proglang{R} packages. These functions have been useful in our daily work at the German Cancer Research 
Center where prognostic modelling of patient survival data is of particular interest. Therefore, for illustration purposes we focus on penalized Cox PH regression models in this article. But the \proglang{R} functions are applicable to all types of regression models implemented in the \pkg{glmnet} package.

Computational efficiency is an important requirement of the software to make applications feasible for real-life data analysis tasks in fields such as molecular oncology, where one aims to develop sparse risk prediction models based on very large numbers of molecular features measured with high-throughput technologies such as microarrays or next-generation sequencing. Therefore, we provide functionality to speed up computations, in particular through parallel computing.

We provide \proglang{R} functions to perform stability selection \citep{MeinshausenBuehlmann2010} in a computationally efficient way which allows to select the most stable features at a given Type I error level. We have also implemented an approach to select the optimal parameter combination ($\alpha$, $\lambda$) for elastic net penalties using an interval-search algorithm \citep{froehlich2005} which is often faster and more accurate than a standard grid search \citep{Jones1998}. Another very useful addition for real-life applications of \pkg{glmnet} for building risk-prediction models is the provision of wrapper functions to allow the computation of resampling-based prediction errors within the framework of the \proglang{R} package \pkg{peperr} \citep{Porz2009}. The \pkg{peperr} package makes it computationally feasible to assess the
predictive accuracy of a penalized regression model via resampling methods even for very large-scale applications by employing parallel computing. We also provide the possibility to speed up stability selection by parallel computing using the functionalities of the \proglang{R} base package \pkg{parallel}.

The software is available as \proglang{R} package \pkg{c060} on R-forge (URL \url{http://c060.r-forge.r-project.org}). 

\section{Data application}
Throughout this article we use the gene expression data set of cytogenetically normal acute myeloid leukemia (AML) patients by \citet{metzeler08} and corresponding clinical data in order to illustrate a typical application of penalized Cox PH regression models, where the aim is to develop a prognostic model for patient survival while at the same time identifying the most influential gene expression features. To simulate the typical situation that only one data set is available for model training and evaluation, we only use the data set that was used as validation data in the original publication by \citet{metzeler08}. The data can be accessed from the Gene Expression Omnibus (GEO) data repository (\url{http://www.ncbi.nlm.nih.gov/geo}) by the National Center for Biotechnology Information (NCBI). We find the Metzeler \textit{et al.} data set under GEO accession number GSE12417.

The data set contains gene expression data for 79 patient samples measured with Affymetrix HG-U133 Plus 2.0 microarrays. The median survival time of these patients was 17.6 months with a censoring rate of 40\%.

\section{Methods and algorithms}

\subsection{Penalized generalized linear models and Cox models}\label{methods:penalized}
An efficient implementation for fitting generalized linear models and Cox proportional hazards models with regularization by the lasso or elastic net penalty terms is provided by the \proglang{R} package \pkg{glmnet} \citep{FHT2010, simon2011}. This implementation uses a coordinate descent algorithm for fitting the models for specified values of penalty parameters $\lambda > 0$ and $\alpha \in (0,1]$. The computation of an entire regularization path across a range of values $\Lambda = \{\lambda_1,\lambda_2...,\lambda_K\}$ at fixed $\alpha$ with \pkg{glmnet} is generally very fast, because previously computed solutions for $\{\lambda_1,...,\lambda_{k-1}\}$ are used as 'hot' starting values for the computation of $\lambda_k$. This implies that it is often more efficient to compute the models for the entire regularization path of $\Lambda$ rather than just individual models. We use this feature in all of our implemented algorithms to make most use of the computational speed of \pkg{glmnet}.

Models are fitted by maximizing the penalized log-likelihood function for generalized linear models and the penalized partial log-likelihood for Cox models. The penalized (partial) log-likelihood function is given by
\begin{equation}\label{pen.part.likelihood}
l_n(\boldsymbol\beta)- \sum_{j=1}^p  p_{\alpha,\lambda}(|\beta_j|)
\end{equation}
where $l_n(\boldsymbol\beta)$ denotes the (partial) log-likelihood given $n$ observations.
The dimension of the parameter vector $\boldsymbol\beta$ is $p$ and $p_{\alpha,\lambda}(|\cdot|)$ is the penalty function with tuning parameters $\lambda$ and $\alpha$.

Cross-validation can be performed to decide which model, i.e. which penalty parameter values, to choose by using the negative cross-validated penalized (partial) log-likelihood as the loss function. Actually, within the \pkg{glmnet} package, the penalized (partial) log-likelihood deviance is used as the loss function rather than the log-likelihood function itself. The deviance is equal to -2 times the log-likelihood ratio of the model of interest compared to the saturated model, which has one free parameter per observation. Obviously, both versions will result in the same optimization result.

\subsection{$L_2$-penalized Cox regression}
Penalized maximum likelihood estimation in Cox regression with the ridge penalty
\begin{equation}\label{ridge}
p_{\lambda}(|\beta_j|)=\lambda\beta_j^2
\end{equation}
was introduced by \citet{verweij94}.
The ridge penalty results in parameter estimates that are biased towards zero, but does not set values to exactly zero, and hence does not perform feature selection.
On the other hand, it has been found to produce models with good prediction performance in high-dimensional genomic applications \citep[e.g.][]{bovelstad07}, in particular if predictors are highly correlated.

\subsection{$L_1$-penalized Cox regression}
\Citet{tibshirani97} proposed to use an $L_1$-penalized Cox model with
\begin{equation}
p_{\lambda}(|\beta_j|)=\lambda | \beta_j |
\end{equation}
and described a technique, called the lasso for ``least absolute shrinkage and selection operator'', for parameter estimation.
The $L_1$-penalty has the advantage over the $L_2$-penalty of shrinking some of the coefficients to zero, i.e. it performs automatic feature selection.

\subsection{The elastic net}
\Citet{zou05} introduced the elastic net, which employs a combination of the $L_1$- and $L_2$-penalty
\begin{equation}\label{enet}
p_{\lambda_1,\lambda_2}(|\beta_j|)=\lambda_1 | \beta_j | + \lambda_2 \beta_j ^2.
\end{equation}
\Citet{zou05} rescale the initial solutions from the optimization of the doubly-penalized log-likelihood function by the factor $1 + \lambda^2$, in order to reduce the effect of the double shrinkage.
Like lasso the elastic net performs automatic feature selection by setting some coefficient estimates to zero.
But the additional $L_2$-penalty term distributes the weight to more features, such that the elastic net tends to select more features than the lasso. 
This is especially the case in situations with high correlation, where the lasso would select only one feature of a set of highly correlated features, while the ridge penalty would give them equal weight.

Throughout this manuscript we use an alternative parametrization of the elastic net penalty function equivalently to the formulation used in the \pkg{glmnet} package:
\begin{equation}\label{enet2}
p_{\alpha,\lambda}(|\beta_j|)= \lambda\times (\alpha |\beta_j| + (1-\alpha)\frac{1}{2} \beta_j^2).
\end{equation}
Here, $\alpha \in (0,1]$ determines the influence of the $L_1$ penalty relative to the $L_2$ penalty. Small $\alpha$ values will result in models with many features, getting closer to the non-sparse ridge solution as $\alpha$ tends to zero.

\subsubsection{The interval-search algorithm to select the optimal elastic net parameter combination}
The elastic net penalty function contrains two tuning parameters which are data-dependent and hence cannot be set to some \emph{a priori} values. The challenge is to find a set of tuning parameters $\left(\alpha,\lambda\right)$, for which the k-fold cross-validated loss function of the model is minimal.


\paragraph{Interval search}
The standard used fixed grid search has its major disadvantage in the systematic check of the penalized log likelihood deviance in each point of the grid. The grid density affects the accurancy and the time complexity of the algorithm.  Furthermore, the choice of the grid is highly arbitrary and does guarantee that the algorithm will not get stuck in a local optimum. 

\Citet{froehlich2005} proposed an 
efficient algorithm for finding a global optimum on the tuning parameter space 
called Efficient Parameter Selection via Global Optimization (EPSGO). The main idea of the algorithm is to treat the task of finding the optimal tuning parameter values as a global optimization problem. For that purpose one learns a Gaussian process model of the loss function surface in parameter space and samples systematically at points where the so-called expected improvement criterion reaches the maximum. 


The interval search can be divided into two phases. In the initial phase, a set of uniformly distributed points is randomly selected throughout the parameter space. Then, in the iteration phase, the algorithm learns the Gaussian process model from the points which have already been visited. By adding new points one updates the Gaussian process model.
New points in the parameter space are sampled by using the expected improvement criterion as described by \citet{Jones1998}. The EPSGO algorithm stops when one of the stopping criteria is met. 
The stopping criteria is either convergence of the algorithm or no change of the optimum during the last ten iterations. THe EPSGO algorithm has been implemented in a R package \pkg{penalizedSVM} \citet{becker2009}.


\Citet{froehlich2005} showed that the algorithm is robust against local minima. 
One can observe an immense improvement in the training time for the Gaussian process model compared to that of the underlying regression task \citep{froehlich2005}. This is because the number of training points for the Gaussian process (and hence the number of evaluations of the loss function surface of the regression model) mainly depends on the dimensionality of the parameter space, which is very small compared to the number of training points on the grid for the regression model.

Summing up,  the EPSGO algorithm provides two main advantages: robustness against local minima and time improvement in comparison to a dense grid search.

\subsection{Stability selection}
The penalized regression models that we have described above are typically used to find sparse models with good predictive performance. In contrast, the stability selection proposed by \citet{MeinshausenBuehlmann2010} aims to find stable features which show strong association with the outcome. 
The stability selection is a general approach that combines feature selection methods such as $L_1$ penalized models with resampling. By applying the corresponding feature selection method to subsamples that were drawn without replacement, selection probabilities for each feature can be estimated as the proportion of subsamples where the feature is included in the fitted model. These selection probabilities are used to define a set of stable features. \citet{MeinshausenBuehlmann2010} provide a theoretical framework for controlling Type I error rates of falsely assigning features to the estimated set of stable features. The selection probability of each feature along the regularization path, e.g. along the range of possible penalization parameters $\Lambda=\{\lambda_1,\lambda_2...,\lambda_K\}$, is called stability path.
Given an arbitrary threshold $\pi_{thr} \in (0.5,1)$ and the set of penalization parameters $\Lambda$, the set of stable features estimated with stability selection is:
\begin{equation} 
 \hat{S}_{\beta}^{stable}=\left\{j: \max_{\lambda_{k} \in \Lambda} \hat{\Pi}_{j}^{\lambda_{k}} \geq \pi_{thr} \right\},
\end{equation}
where $\hat{\Pi}_{j}^{\lambda_{k}}$ denotes the estimated selection probability of the $j$th feature at $\lambda_k$. 
Then according to Theorem 1 in \citet{MeinshausenBuehlmann2010}, the expected number of falsely selected features $E(V)$ will be bounded by:
\begin{equation}\label{stab}
E(V) \leq \frac{1}{(2\pi_{thr}-1)} \frac{q^{2}_{\Lambda}}{p},
\end{equation}
where $q_{\Lambda}$ is the average of the number of non-zero coefficients w.r.t. to the drawn subsamples.  
Interpreting Equation \ref{stab} the expected number of falsely selected features decreases by either reducing the average number of selected features $q_{\Lambda}$ or by increasing the threshold $\pi_{thr}$. Suppose that $\pi_{thr}$ is fixed, then $E(V)$ can be controlled by limiting $q_{\Lambda}$ by the length of the regularization path $\Lambda$. In multiple testing the expected number of falsely selected features is also known as the per-family error rate (PFER) and if divided by the total number of features $p$ will become the per-comparison error rate (PCER) \citep{Dudoit2003}. The stability selection allows to control these Type I error rates. For instance, suppose the threshold $\pi_{thr} = 0.8$ is fixed, then choosing $\Lambda$ such that $q_{\Lambda} \leq \sqrt{0.6p}$ will control $E(V)\leq 1$. Moreover, by choosing $\Lambda$ so that $q_{\Lambda} \leq \sqrt{0.6p\alpha}$ will control the family wise error rate (FWER) at level $\alpha$, $P(|V|>0) \leq \alpha$.

As mentioned before, according to \citet{FHT2010} the coordinate descent algorithm implemented in the \pkg{glmnet} package is most efficient regarding the computational time, when used to calculate a whole regularization path. To utilize this property our algorithm calculates the stability path by first generating subsets by subsampling and then calculating for each subsample the regularization path using the coordinate descent algorithm. The resulting regularization paths are then averaged to form the stability path. Furthermore, since the calculations of the regularization paths for each subset are independent of each other, the algorithm can easily be parallelized using the package \pkg{parallel}. %In parallel computing such easily to parallelize  embarrassingly parallel computations.    

\subsection{Prediction error curves for survival models}

The time-dependent Brier score \citep{graf99} can be used to assess and compare the prediction accuracy of prognostic models.
The Brier score at time point $t$ is a weighted mean squared error between predicted survival probability and observed survival status.
Weighting depends on the estimated censoring distribution to account for the observations under risk \citep{gerds2006}. Computing the error for each time point over the entire follow-up horizon yields a predicton error curve. As a reference we use prediction errors based on the Kaplan-Meier curves estimated without any covariate information. 

The empirical time-dependent Brier score $BS(t)$ is defined as a function of time $t > 0$ by
\[
BS(t) = \frac{1}{n}\sum_{i=1}^{n}{\left[\frac{\hat{S}(t|x_i)^2I(t_i\leq t \wedge \delta_i=1)}{\hat{G}(t_i)} + \frac{(1-\hat{S}(t|x_i))^2I(t_i>t)}{\hat{G}(t)}\right]},
\]
with individual survival time $t_i$, censoring indicator $\delta_i$ and estimated survival probability $\hat{S}(t|x_i)$ at time $t$ based on the prognostic model given covariate values $x_i$ for subject $i$ out of $n$ patients \citep{graf99}.
$\hat G(t)$ denotes the Kaplan-Meier estimate of the censoring distribution at time $t$, which is based on the observations $(t_i; 1-\delta_i$), $i = 1,...,n$. $I$ stands for the indicator function. 

In case no independent validation data are available, resampling-based prediction error curves are used to adequately assess the model's prediction accuracy. The $.632+$ bootstrap estimator \citep{efron1997} is commonly used for these applications, which is a weighted sum of the apparent error and the average out-of-bag bootstrap error. For the apparent error  the same data is used to develop the prognostic model and assess its performance. Due to overfitting, this error is far too optimistic, particularly with a high-dimensional covariate space. The average out-of-bag bootstrap error is too conservative since only a proportion of the entire data is used to develop the prognostic model in each bootstrap run. 
The $.632+$ estimator balances both estimators, and additionally accounts for the relative overfitting based on the no-information error rate.
Further, in our application the $.632+$ bootstrap estimator is calculated based on subsampling (with replacement) rather than classical sampling wihout replacement, as that has been demonstrated to lead to more accurate estimates in a high-dimensional context \citep{BS2008}.

<<setup, echo=F, results=hide>>=
rm(list=ls())
options(prompt = "R> ", continue = "+  ", width = 70, useFancyQuotes = FALSE)
#setwd("./vignettes")
# load libraries
library(cacheSweave)
library(Biobase)
library(genefilter)
#library(GEOquery)
library(limma)
library(xtable)
library(survival)

library(glmnet)
require(penalizedSVM)
library(parallel) #for stabilityselection.R
library(peperr) #for peperr_glmnet.R
library(tgp) #for EPSGO.R
library(mlegp) #for EPSGO.R
library(pamr) #for tune.glmnet.interval.R
library(lattice) # for plotting 

library(c060)

dir.create("figures",showWarnings = FALSE)

# set cache folder; needs different driver, to run as:
# Sweave("c060_vignette.Rnw",driver=cacheSweaveDriver)
setCacheDir("cache")
@

<<GEOdataGSE12417, echo=F, results=hide>>=
if (file.exists(".Rdata/expressionSet.Rdata")) {
    load(".Rdata/expressionSet.Rdata") 
} else {
    library(GEOquery)
    
    dir.create(".GEOdata",showWarnings = FALSE)
    dir.create(".Rdata",showWarnings = FALSE)
    
    # download processed data files from url or if available get data from locally saved copies
    geo_exprs_data  <- getGEO("GSE12417", destdir=".GEOdata",AnnotGPL=T)[[1]]
    annotation(geo_exprs_data) <- "hgu133plus2"
    
    #clinical characteristics including survival data are in "characteristics_ch1" in the phenoData 
    clin.df <- as.data.frame(strsplit2(as.character(pData(geo_exprs_data)$characteristics_ch1), split=";"))
    clin.df[,1] <- strsplit2(clin.df[,1]," ")[,7]
    clin.df[,2] <- as.numeric(sub("=", "", strsplit2(clin.df[,2]," ")[,3]))
    clin.df[,3] <- as.numeric(strsplit2(clin.df[,3]," ")[,4])
    clin.df[,4] <- as.numeric(strsplit2(clin.df[,4]," ")[,4])
    colnames(clin.df) <- c("FAB", "age", "os", "os_status")
    rownames(clin.df) <- rownames(pData(geo_exprs_data))
    pData(geo_exprs_data) <- clin.df

    save(geo_exprs_data, file=".Rdata/expressionSet.Rdata")  
    unlink(".GEOdata", recursive=TRUE) #delete GEOdata
} 
@

<<preprocessData, echo=F, results=hide>>=
# unspecific filtering: select most varying probe sets
nfeatures <- 10000
rowvars    <- rowVars(exprs(geo_exprs_data))
topprobes   <- which(rowvars>=sort(rowvars,decreasing=T)[nfeatures])
eset       <- geo_exprs_data[topprobes,]     
@

\clearpage
\section{Application and demonstration of software}
In the following we will demonstrate the use of the functions provided with the \pkg{c060} package in an application to the acute myeloid leukemia (AML) data set by \citet{metzeler08}. For the sake of convenience we reduce the total number of \Sexpr{nrow(geo_exprs_data)} gene expression features that have been measured with the Affymetrix HG-U133 Plus 2.0 microarray technology to the top \Sexpr{nfeatures} features with largest variance across all \Sexpr{ncol(eset)} samples. For all computations the data set is stored as an \code{ExpressionSet} (from Bioconductor package \pkg{Biobase} \citep{Bioconductor}) called \code{eset}. The gene expression data matrix can be accessed through the call \code{exprs(eset)} and overall survival data and other patient-specific data (e.g. patient age) are stored within the \code{phenoData} object \code{pData(eset)}.

\subsection{Starting off: Fitting the lasso-penalized Cox model} 
Our goal is to develop a prognostic model for patient overall survival based on the gene expression data. The purpose of this modelling exercise is not just to fit a prognostic model that is capable of predicting overall survival rates, but we also want to find out which gene expression features are most relevant for this task. Traditionally, this problem is solved by feature selection methods and we start our data analysis exercise by fitting the lasso-penalized Cox model, which provides automatic feature selection.

\begin{figure}
\begin{center}
<<glmnet, echo=F, results=hide, fig=TRUE, height=4>>=
set.seed(1234)
cvres <- cv.glmnet(y=Surv(pData(eset)$os, pData(eset)$os_status), x=t(exprs(eset)), family="cox", nfolds=10)
res <- cvres$glmnet.fit
plot(cvres)

cof <- coef(res, s=cvres$lambda.min)
names.cof <- rownames(cof)
cofn <- cof[which(cof!=0)]
names(cofn) <- names.cof[which(cof!=0)]
@
\end{center}
\caption{\label{fig:cvlasso}Cross-validated partial log-likelihood deviance, including upper and lower standard deviations, as a function of $\log{\lambda}$ for the AML data set. The dotted vertical lines indicate the $\lambda$ values with minimal deviance (left) and with the largest $\lambda$ value within one standard deviation of the minimal deviance (right).}
\end{figure}

We can apply the \code{glmnet} function to fit a lasso-penalized Cox model to the AML data set. The function call with default penalty parameter settings will fit the lasso model for 100 $\lambda$ values with a data-derived range of values:
<<glmnet0, echo=TRUE, results=hide>>=
fit <- glmnet(y=Surv(pData(eset)$os, pData(eset)$os_status), 
              x=t(exprs(eset)), family="cox")
@
 
In order to determine the optimal lasso penalty parameter value, we perform 10-fold cross-validation using the \code{cv.glmnet} function. The loss function, i.e. the cross-validated partial log-likelihood deviance, is shown in Figure \ref{fig:cvlasso} including upper and lower standard deviations as a function of $\log{\lambda}$ for the AML data set. The penalty parameter value minimizing the loss function is $\lambda =$\Sexpr{round(cvres$lambda.min, 3)} ($\log\lambda =$\Sexpr{round(log(cvres$lambda.min), 3)}) and corresponds to a final lasso model with the following \Sexpr{length(cofn)} selected features and lasso regression coefficient estimates:
<<glmnet2, echo=F, results=verbatim>>=
print(cofn, digits=3)
cofn.lasso <- cofn
#cofn.ix <- predict(res, type="nonzero", s=cvres$lambda.min)
@

The selected features are highlighted as red lines in the coefficient paths shown in Figure \ref{fig:coefpath} illustrating the development of the regression coefficient estimates with increasing regularization. While the selected \Sexpr{length(cofn)} features are the only features selected at the optimal $\lambda$ value, they do not remain among the features with largest effect sizes when the penalty is reduced and thus more and more coefficients start to enter the model. In fact, for 4 out of the \Sexpr{length(cofn)} features the coefficient estimates go back down to zero for small values of $\log\lambda$, indicating that these features get replaced by other gene expression features in very large models.

\begin{figure}
\begin{center}
<<glmnet3, echo=F, results=hide, fig=TRUE, height=7, width=7>>=
bet <- res$beta[match(names(cofn), rownames(res$beta)),]
par(mar=c(4,4,2.5,1), mgp=c(2.5,1,0), mfrow=c(2,2))

plot(res, xvar="lambda", col="gray")
glmnet:::plotCoef(bet, lambda = res$lambda, df = res$df, dev = res$dev.ratio, xvar = "lambda", add=TRUE, col="red")
abline(v=log(cvres$lambda.min), lty=3)
abline(v=log(cvres$lambda.1se), lty=3)

glmnet:::plotCoef(bet, lambda = res$lambda, df = res$df, dev = res$dev.ratio, xvar = "lambda", add=FALSE, col="red")
abline(v=log(cvres$lambda.min), lty=3)
abline(v=log(cvres$lambda.1se), lty=3)

norm <- apply(abs(res$beta), 2, sum)
plot(res, xvar="norm", col="gray")
glmnet:::plotCoef(bet, xvar = "norm", add=TRUE, col="red",
                  norm = norm, lambda = res$lambda, df = res$df, dev = res$dev.ratio)
abline(v=norm[match(cvres$lambda.min, cvres$lambda)], lty=3)
abline(v=norm[match(cvres$lambda.1se, cvres$lambda)], lty=3)

plot(res, xvar="dev", col="gray")
glmnet:::plotCoef(bet, lambda = res$lambda, df = res$df, dev = res$dev.ratio, xvar = "dev", add=TRUE, col="red")
abline(v=res$dev.ratio[match(cvres$lambda.min, cvres$lambda)], lty=3)
abline(v=res$dev.ratio[match(cvres$lambda.1se, cvres$lambda)], lty=3)
@
\end{center}
\caption{\label{fig:coefpath}Coefficient paths for lasso-penalized Cox PH regression models applied to the AML data set. The features with highlighted paths have non-zero coefficients in the model with the optimal $\lambda$ value as determined by ten-fold cross-validation. The dotted vertical have the same meaning as in Figure \ref{fig:cvlasso}. The top plots show the coefficient path scaled to reflect $\log(\lambda)$ on the x-axis (top left: full path, top right: zoomed in to only show the selected features). The bottom plots show the coefficient paths relative to the $L_1$-norms of the estimated coefficient vector (left) and to the fraction of the null partial log-likelihood deviance explained (right).}
\end{figure}

\subsection{Assessment of prediction performance with resampling-based prediction errors}
\label{pec}

Once the final prognostic model is selected, we need to assess its prediction accuracy for future patients, frequently also in comparison
with established clinico-pathological prognostic markers. In many applications no independent validation data set is available, thus the same data set needs to be used to both develop and assess the prognostic model. This is even more problematic for high-dimensional data,
where the risk of overfitting is especially high. Resampling-based methods can be used to unbiasedly estimate the predictive accuracy 
of the prognostic model in this situation. This is also called internal validation.

For this purpose the \proglang{R} package \pkg{peperr} \citep{Porz2009} provides a modular framework for survival and binary endpoints, i.e. prognostic and
classification models. Wrapper functions for new or customized prediction model algorithms can be defined and passed to the generic call function \code{peperr}.  
In case of prognostic models for survival endpoints, algorithm-specific wrapper functions are required for model fitting, tuning and prediction. Wrapper functions for selected machine learning approaches are already implemented, but not yet for the \pkg{glmnet} package.

With the \pkg{peperr} package prediction accuracy of survival models is by default assessed with prediction error curves based on the time-dependent Brier score, but it is also possible to define and use customized accuracy measures. We have implemented additional wrapper functions for the \pkg{glmnet} algorithm for fitting (\code{fit.glmnet}) and tuning (\code{complexity.glmnet}) the model, and predicting survival probabilities (\code{predictProb.glmnet}) based on the fitted model and the estimated baseline hazard from the training data. We here want to assess the prognostic value of the $L_1$-penalized Cox PH regression model fitted in the previous section. The $.632+$ subsampling-based bootstrap estimator is calculated using 1000 bootstrap samples. The \pkg{peperr} package is designed for high-dimensional covariates data and allows for various set-ups of parallel computations.
Also, additional arguments can be passed directly to the \pkg{glmnet} call by specifing additional arguments for the corresponding fitting and/or tuning procedure.
Here, we include patient's age as mandatory model feature into the prognostic model, i.e. age is not subject to penalization, and 
run the calculation on 3 CPUs in parallel using a socket cluster set-up. 

<<peperrMandatoryParallel, echo=T, results=hide, cache=T>>=
obj  <- peperr(response=Surv(pData(eset)$os, pData(eset)$os_status),
               x=data.frame(eset$age,t(exprs(eset))),
               fit.fun=fit.glmnet, args.fit=list(standardize=F, family="cox",
               penalty.factor=rep(0:1, times=c(1,dim(eset)[1]))),
               complexity=complexity.glmnet, 
               args.complexity=list(standardize=F, nfolds=10, family="cox", 
               penalty.factor=rep(0:1, times=c(1,dim(eset)[1]))),
               RNG="fixed", seed=0815, cpus=3, parallel=T, clustertype="SOCK", 
               load.list=list(packages=c("c060")), 
               indices=resample.indices(n=dim(eset)[2], sample.n=10, method="sub632"))
@

Individual bootstrap results can be visualized with the \code{plot.peperr} function from the \pkg{peperr} package showing the selected complexity parameters, 
out-of-bag prediction error curves as well as the prediction error integrated over time, and the predictive partial log-likelihood (PLL) values. In order to calculate the predictive PLL values again an algorithm specific wrapper (here \code{PLL.coxnet}) needs to be defined. In addition, we provide a slightly modified version of the prediction error curves plot function from the \pkg{peperr} package which allows to display the number still at risk and pointwise bootstrap quantiles (\code{plot.peperr.curves}) as shown in Figure \ref{fig:pec}. By default, the $.632+$ is calculated and displayed. Displaying also the $.632$ estimator, the no-information error rate and the average out-of-bag bootstrap error is optional in \code{plot.peperr.curves}. 

\begin{figure}
\begin{center}
<<peperrPlot, echo=T, results=hide, fig=TRUE>>=
plot.peperr.curves(obj)
@
\end{center}
\caption{Prediction error curves based on time-dependent Brier score for the lasso-penalized Cox PH regression model applied to the AML data set (evaluation time points reflect days). The gray area indicates the pointwise 2.5\% and 97.5\% quantiles of the 1000 out-of-bag bootstrap samples. The other lines show the prediction error curves of the null model (estimated by the Kaplan-Meier estimator without covariate information), the full apparent error estimates (i.e. the errors as estimated when applying the model to the entire training data set), and the .632+ bootstrap error estimates.}
\label{fig:pec}
\end{figure}

Note, that for classification models, the same wrapper functions for fitting and tuning the model are called.
Model performance measures for classification tasks shipped with the \pkg{peperr} package are misclassification rate and Brier score. We have extended the functionality of the Brier score (\code{aggregation.brier}) and misclassification rate (\code{aggregation.misclass}) 
calculation for the \pkg{glmnet} algorithm, and defined AUC under the ROC curve (\code{aggregation.auc}) as an additional performance measure.
For binary responses, the \pkg{peperr} package does not quite provide the same modular flexibility 
as for time-to-event endpoints. The predicted class probability is calculated within the generic performance/aggregation function by calling the algorithm-specific predict function. Whenever a new algorithm is incorporated, the generic aggregation function needs to be adapted accordingly.

\clearpage
\subsection{Stability selection}

We use stability selection to identify prognostic features which have a relevant influence on the survival times of the patients in the AML data set and to control Type I errors to ensure that the features identified are truly associated with the survival times. To calculate the stability path for the $L_1$-penalized Cox regression we use the function \code{stability.path} from our \proglang{R} package. 
The function draws subsets and calculates in parallel the stability path, e.g. the selection probabilities of each feature along the range of possible penalization parameter values. For parallelization we use the package \pkg{parallel}, which has been a base package since \proglang{R} version 2.14.0. On Unix-like systems the parallelization is done by forking via the function \code{mclapply} whereas under Windows systems socket clusters are used. 

<<stabilitySelection1, echo=T, results=verbatim>>=
set.seed(1234)
y <- cbind(time=pData(eset)$os, status=pData(eset)$os_status)
spath <- stability.path(y=y, x=t(exprs(eset)), mc.cores=2, family="cox")
@

The function \code{stability.selection} can be called to estimate the stable set of features. Controlling a family-wise error rate (FWER) of $0.5$ the estimated set of stable features comprises a single feature (with $\hat\Pi>0.6$).

<<stabilitySelection2, echo=T, results=verbatim>>=
stability.selection(spath,fwer=0.5)$stable
@

Note that an FWER threshold of 0.5 is unusually high. For this data set the use of smaller, more conventional thresholds, e.g. 0.05 or 0.1, will result in an empty set of stable features. The stability path can be visualized by the function \code{plot.stabpath}, which will call \code{stability.selection} to identify stable features and highlight them in the plot.

\begin{figure}
<<stabilitySelection3, echo=T, results=hide, fig=TRUE>>=
plotstabpath(spath, fwer=0.5, pi_thr=0.6, xvar="lambda", col.all="gray")
@
\caption{\label{fig:stabpath}Coefficient and stability paths for lasso penalized Cox PH regression model applied to the AML data set. The feature with highlighted path is the only stable feature found by stability selection with FWER=0.5 and $\hat\Pi >0.6$.}
\end{figure}

\clearpage
\subsection{Parameter tuning for the elastic net Cox model}
In the previous sections we have seen that the lasso-penalized Cox model does not seem to perform very well in terms of predicting overall survival for the AML data set. 
The lasso model identified as the optimal model by 10-fold cross-validation is very sparse and contains only \Sexpr{length(cofn)} features.
Furthermore, we have observed that these features are not very stable and 4 out of them do not even remain in the set of selected features when the amount of regularization is decreased and more features start to enter the model. Thus, the $.632+$-bootstrap estimated prediction error curve indicates that the prognostic value of the lasso model does not seem to be very good.

In this section we fit an elastic net model instead of lasso to the same data set. As outlined above, fitting an elastic net model require the simultaneous tuning of two parameters $\alpha$ and $\lambda$. For this computationally challenging task, we provide an efficient implementation in \proglang{R} function \code{EPSGO} using the interval search algorithm.

<<interval_search_cox, echo=FALSE, results=hide>>=
if (file.exists(".Rdata/fit_interval_search.RData")) {
    load(".Rdata/fit_interval_search.RData") 
} else {
  x <- t(exprs(eset))
  y <- cbind(time=pData(geo_exprs_data)$os,status=pData(geo_exprs_data)$os_status)

  # bounds for alpha [0,1]
  bounds <- t(data.frame(alpha=c(0, 1)))
  colnames(bounds) <- c("lower", "upper")  

  nfolds = 10
  # fix folds for each model
  my.seed<-1234
  set.seed(my.seed)
  foldid <- my.balanced.folds(class.column.factor=y[,2], cross.outer=nfolds)
  
  print("start interval search")
  fit <- EPSGO(Q.func="tune.glmnet.interval", 
             bounds=bounds, 
             parms.coding="none", 
             show = "none", 
             N = NULL,   
             seed = my.seed, 
             fminlower = -100,
             # Q.func arguments
             x = x, y = y, family = "cox", 
             foldid = foldid,
             type.min = "lambda.1se",
             type.measure = "deviance",
             verbose = TRUE)
  #names(fit)
  save(fit,  file=".Rdata/fit_interval_search.RData") 
}  


@

The task is to find a setting of tuning parameter values $\left(\alpha,\lambda\right)$, for which the \Sexpr{fit$model.list[[1]]$model$nfolds}-fold cross-validated penalized (partial) log likelihood function of the model is minimal.  The wrapper function \\
\code{tune.glmnet.interval} calculates partial log likelihood deviance of a model with given tuning parameter setting $\left(\alpha,\lambda\right)$. The parameter space of tuning parameter  $\alpha$ is an interval $\left(0,1\right)$.

The second tuning parameter $\lambda>0$ will be found for each given $\alpha$ via the computation of the entire regularization path with the \code{glmnet} function. Thus, the two-dimensional parameter space for tuning parameters $\left(\alpha,\lambda\right)$ has the form $\left(0, 1\right) \times \mathbb{R}_+$.
For each given $\alpha$ an optimal $\lambda$ is defined as the largest value of $\lambda$ such that the loss function value is within one standard error of the minimum (\texttt{type.min ='lambda.1se'}).


<<interval_search_cox_show, echo=TRUE, eval=FALSE>>=
  fit <- EPSGO(Q.func="tune.glmnet.interval", 
             bounds=bounds, 
             parms.coding="none", 
             seed = 1234, 
             fminlower = -100,
             # Q.func arguments
          	 x = x, y = y, family = "cox", 
             foldid = foldid,
             type.min = "lambda.1se",
             type.measure = "deviance")
@

Summary information can be extracted from the \texttt{fit} object using  \code{summary.int.search} function.
<<interval_search_cox_extract, echo=TRUE, results=hide>>=
summary.int<-summary.int.search(fit,verbose=TRUE)
@

At the initial step we sample 21 points in the parameter space. Those points are randomly distributed and uniformly cover the whole interval [0,1]. A Gaussian process model is trained based on these initial points. Then, iteratively, new points are added to the Gaussian process model in order to find an optimal combination of tuning parameter values. In total, \Sexpr{nrow(summary.int$info)} iterations were needed to reach the optimum. 



<<interval_search_cox_fit_opt, echo=FALSE,results=hide>>=
#select the model with optimal parameters from the object fit
#
opt.model<-summary.int$opt.models[[1]]
cofn<-get.cofn.int.search(opt.model)
@

The final elastic net model contains \Sexpr{length(cofn)} selected features, which obviously reflects much less sparsity than the final lasso model.
The results are consistent in the sense, that the features contained in the final lasso model are also contained in the elastic net model. Also, the individual feature selected by the stability algorithm is in the set of selected elastic net features.
<<interval_search_cox_check_overlap_stability,echo=FALSE,results=hide>>=
names(cofn.lasso) %in% names(cofn)
'206932_at' %in% names(cofn)
@

\begin{figure}
<<interval_search_cox_out_plot1, echo=F, results=hide, fig=TRUE,height=8,width=8>>=
#c060:::plot.summary.int.search(summary.int)
plot.summary.int.search(summary.int)
@
\caption{\label{fig:interval_search1} Partial log likelihood deviance as a function of both tuning parameters $\alpha$ and log$\lambda$ when fitting the elastic net Cox model for the AML data set. For each evaluated point in the parameter space the number of selected features in the corresponding model is printed next to the data point symbol. Rectangles correspond to initially selected $\alpha$ values. The dashed red lines highlight the optimal point with minimal partial log likelihood deviance. }
\end{figure}

Figure~\ref{fig:interval_search1} illustrates the relationship between both tuning parameters $\alpha$ and $\lambda$ for the 'visited' points in the parameter space. The partial log likelihood deviance is  color-coded with black for small values and gray for large values.  The number of features selected  inthe corresponding model is written near each point. 
To distinguish between initial and iteration points, the initial points are plotted as  squares and iteration points as circles. One can observed that the iteration points were chosen in the regions with lower deviance values.

The distribution of initial points (iteration=0) and visited points (iteration>0) in the parameter space is plotted in Figure~\ref{fig:interval_search2}.
This plot shows nicely that the interval search algorithm does not sequentially cover the entire parameter space, but rather quickly finds promising regions and draws new samples there. The optimal model contains with mininmal log-likelihood dieviance is found for $\alpha$ =\Sexpr{summary.int$opt.alpha} $\log\lambda$ = \Sexpr{log(summary.int$opt.lambda)} and highlighted as a vertical line.

\begin{figure}
<<interval_search_cox_out_plot2, echo=FALSE, results=hide, fig=TRUE,height=8,width=8>>=
plot.points.int.search(summary.int) 
@
\caption{\label{fig:interval_search2} The distribution of initial and visited points of the interval search plotted in chronological order. The interval search is employed to identify the optimal parameter value combination ($\alpha$,$\lambda$) for the elastic net Cox model fitted to the AML data set.}
\end{figure}

\clearpage
\section{Conclusions and outlook}
The programming language and statistical computing environment \proglang{R} provides a highly useful framework for statistical data analysis and modelling. It is the dominating statistical software in many areas, for example in molecular biology and molecular medicine, which is largely due to the highly successful Bioconductor project \citep{Bioconductor}, which provides tools for the analysis and comprehension of high-throughput genomic data. Due to the open-source nature of \proglang{R} and Bioconductor, many very useful software packages have been developed by \proglang{R} users and made available for the entire \proglang{R} community. One example is the \pkg{glmnet} package, which  implements an efficient state-of-the-art algorithm for fitting penalized Cox and generalized linear models \citep{FHT2010,simon2011}.

We have presented our \proglang{R} package \pkg{c060}, which provides extensions to \pkg{glmnet} and additional features which are essential for a full data analysis in practical applications, including stability selection, estimation of prediction error (curves) and an efficient interval search algorithm for finding the optimal elastic net tuning parameter combination. These extensions have proved useful in our daily work, in particular for the task of performing prognostic modelling of patient survival data based on high-dimensional molecular biology data.

The \pkg{c060} package will be kept updated in the future to keep up with the fast-developing field of penalized regression methodology for feature selection and risk prediction modelling with high-dimensional input data. One example are developments for the estimation of standard errors, confidence intervals and the determination of p-values in high-dimensional regularized regression models, e.g. through subsampling methods similar to the approach taken by \citet{wasserman2009} and \citet{MeinshausenBuehlmann2010}.


%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

%\section[About Java]{About \proglang{Java}}
%% Note: If there is markup in \(sub)section, then it has to be escape as above.
\clearpage
\bibliographystyle{jss}
\bibliography{c060}   % na
\end{document}
